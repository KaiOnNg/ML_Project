{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('SP500_with_indicators_^GSPC.csv').dropna()\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Create lagged features\n",
    "def create_lagged_features(data, lag=30):\n",
    "    \"\"\"\n",
    "    Generate lagged features for time series prediction.\n",
    "    \"\"\"\n",
    "    df_lagged = data.copy()\n",
    "    for i in range(1, lag + 1):\n",
    "        df_lagged[f'Lag_{i}'] = df_lagged['Adj Close'].shift(i)\n",
    "    df_lagged.dropna(inplace=True)  # Drop rows with NaN values (due to lagging)\n",
    "    return df_lagged\n",
    "\n",
    "# Apply lagged features\n",
    "lagged_df = create_lagged_features(df[['Adj Close']], lag=30)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = lagged_df.drop(columns=['Adj Close']).values\n",
    "y = lagged_df['Adj Close'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    }
   ],
   "source": [
    "# Time Series Cross-Validation \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize Ridge Regression (L2 Regularization)\n",
    "alpha = 0.5\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "\n",
    "# Initialize XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Pre-tune XGBoost hyperparameters using GridSearchCV on full training set\n",
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "X_train_full, y_train_full = X_scaled[:split_idx], y[:split_idx]\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,  # Use 3-fold cross-validation for hyperparameter tuning\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_full, y_train_full)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Parameters for XGBoost:\", best_params)\n",
    "\n",
    "# Use the best hyperparameters for XGBoost in the cross-validation loop\n",
    "optimized_xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)\n",
    "\n",
    "# Initialize CNN\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_length):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 4, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(4)\n",
    "        self.conv2 = nn.Conv1d(4, 8, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.conv3 = nn.Conv1d(8, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.fc = nn.Linear(input_length * 32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation evaluation for all models\n",
    "ridge_mae, xgb_mae, cnn_mae = [], [], []\n",
    "ridge_rmse, xgb_rmse, cnn_rmse = [], [], []\n",
    "ridge_r2, xgb_r2, cnn_r2 = [], [], []\n",
    "fold = 1\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Ridge Regression\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    ridge_pred = ridge_model.predict(X_test)\n",
    "    ridge_mae.append(mean_absolute_error(y_test, ridge_pred))\n",
    "    ridge_rmse.append(np.sqrt(mean_squared_error(y_test, ridge_pred)))\n",
    "    ridge_r2.append(r2_score(y_test, ridge_pred))\n",
    "\n",
    "    # XGBoost\n",
    "    optimized_xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = optimized_xgb_model.predict(X_test)\n",
    "    xgb_mae.append(mean_absolute_error(y_test, xgb_pred))\n",
    "    xgb_rmse.append(np.sqrt(mean_squared_error(y_test, xgb_pred)))\n",
    "    xgb_r2.append(r2_score(y_test, xgb_pred))\n",
    "\n",
    "    # CNN\n",
    "    X_train_cnn = torch.tensor(X_train.reshape(X_train.shape[0], 1, X_train.shape[1]), dtype=torch.float32)\n",
    "    y_train_cnn = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_cnn = torch.tensor(X_test.reshape(X_test.shape[0], 1, X_test.shape[1]), dtype=torch.float32)\n",
    "    y_test_cnn = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    cnn_model = ConvNet(input_length=X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train CNN\n",
    "    cnn_model.train()\n",
    "    for epoch in range(10):  # Use fewer epochs per fold for efficiency\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(X_train_cnn)\n",
    "        loss = criterion(outputs, y_train_cnn)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate CNN\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        cnn_pred = cnn_model(X_test_cnn).numpy().flatten()\n",
    "    cnn_mae.append(mean_absolute_error(y_test, cnn_pred))\n",
    "    cnn_rmse.append(np.sqrt(mean_squared_error(y_test, cnn_pred)))\n",
    "    cnn_r2.append(r2_score(y_test, cnn_pred))\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"  Ridge - MAE={ridge_mae[-1]:.2f}, RMSE={ridge_rmse[-1]:.2f}, R2={ridge_r2[-1]:.4f}\")\n",
    "    print(f\"  XGBoost - MAE={xgb_mae[-1]:.2f}, RMSE={xgb_rmse[-1]:.2f}, R2={xgb_r2[-1]:.4f}\")\n",
    "    print(f\"  CNN - MAE={cnn_mae[-1]:.2f}, RMSE={cnn_rmse[-1]:.2f}, R2={cnn_r2[-1]:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# Average metrics across folds\n",
    "print(\"\\nCross-Validation Results (Average):\")\n",
    "print(f\"Ridge - MAE: {np.mean(ridge_mae):.2f}, RMSE: {np.mean(ridge_rmse):.2f}, R2: {np.mean(ridge_r2):.4f}\")\n",
    "print(f\"XGBoost - MAE: {np.mean(xgb_mae):.2f}, RMSE: {np.mean(xgb_rmse):.2f}, R2: {np.mean(xgb_r2):.4f}\")\n",
    "print(f\"CNN - MAE: {np.mean(cnn_mae):.2f}, RMSE: {np.mean(cnn_rmse):.2f}, R2: {np.mean(cnn_r2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation on Test Set\n",
    "# Train-test split for final evaluation\n",
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# XGBoost\n",
    "optimized_xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = optimized_xgb_model.predict(X_test)\n",
    "\n",
    "# CNN\n",
    "X_train_cnn = torch.tensor(X_train.reshape(X_train.shape[0], 1, X_train.shape[1]), dtype=torch.float32)\n",
    "y_train_cnn = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_cnn = torch.tensor(X_test.reshape(X_test.shape[0], 1, X_test.shape[1]), dtype=torch.float32)\n",
    "y_test_cnn = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "cnn_model = ConvNet(input_length=X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train CNN\n",
    "cnn_model.train()\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = cnn_model(X_train_cnn)\n",
    "    loss = criterion(outputs, y_train_cnn)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    cnn_pred = cnn_model(X_test_cnn).numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation metrics\n",
    "ridge_mae = mean_absolute_error(y_test, ridge_pred)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "cnn_r2 = r2_score(y_test, cnn_pred)\n",
    "\n",
    "# 1% Error Check\n",
    "ridge_accuracy = [abs((pred - actual) / actual) <= 0.01 for pred, actual in zip(ridge_pred, y_test)]\n",
    "ridge_accuracy_rate = sum(ridge_accuracy) / len(ridge_accuracy) * 100\n",
    "\n",
    "xgb_accuracy = [abs((pred - actual) / actual) <= 0.01 for pred, actual in zip(xgb_pred, y_test)]\n",
    "xgb_accuracy_rate = sum(xgb_accuracy) / len(xgb_accuracy) * 100\n",
    "\n",
    "cnn_accuracy = [abs((pred - actual) / actual) <= 0.01 for pred, actual in zip(cnn_pred, y_test)]\n",
    "cnn_accuracy_rate = sum(cnn_accuracy) / len(cnn_accuracy) * 100\n",
    "\n",
    "# Print final evaluation metrics\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "print(f\"Ridge - Accuracy (within 1% error): {ridge_accuracy_rate:.2f}%\")\n",
    "print(f\"Ridge - MAE: {ridge_mae:.2f}, RMSE: {ridge_rmse:.2f}, R2: {ridge_r2:.4f}\")\n",
    "print(f\"XGBoost - Accuracy (within 1% error): {xgb_accuracy_rate:.2f}%\")\n",
    "print(f\"XGBoost - MAE: {xgb_mae:.2f}, RMSE: {xgb_rmse:.2f}, R2: {xgb_r2:.4f}\")\n",
    "print(f\"CNN - Accuracy (within 1% error): {cnn_accuracy_rate:.2f}%\")\n",
    "print(f\"CNN - MAE: {cnn_mae:.2f}, RMSE: {cnn_rmse:.2f}, R2: {cnn_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "# Ridge Regression: Actual vs Predicted\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(range(len(y_test)), y_test, label='Actual', color='blue')\n",
    "plt.plot(range(len(ridge_pred)), ridge_pred, label='Ridge Predicted', color='red')\n",
    "plt.title(f'Ridge Regression (L2 Regularization): Actual vs Predicted Prices')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# XGBoost: Actual vs Predicted\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(range(len(y_test)), y_test, label='Actual', color='blue')\n",
    "plt.plot(range(len(xgb_pred)), xgb_pred, label='XGBoost Predicted', color='green')\n",
    "plt.title(f'XGBoost: Actual vs Predicted Prices')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# CNN: Actual vs Predicted\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(range(len(y_test)), y_test, label='Actual', color='blue')\n",
    "plt.plot(range(len(cnn_pred)), cnn_pred, label='CNN Predicted', color='orange')\n",
    "plt.title(f'CNN: Actual vs Predicted Prices')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 1% Error Accuracy for All Models\n",
    "plt.subplot(4, 1, 4)\n",
    "ridge_accuracy_plot = [1 if abs((p - a) / a) <= 0.01 else 0 for p, a in zip(ridge_pred, y_test)]\n",
    "xgb_accuracy_plot = [1 if abs((p - a) / a) <= 0.01 else 0 for p, a in zip(xgb_pred, y_test)]\n",
    "cnn_accuracy_plot = [1 if abs((p - a) / a) <= 0.01 else 0 for p, a in zip(cnn_pred, y_test)]\n",
    "\n",
    "plt.plot(ridge_accuracy_plot, label='Ridge Accuracy (1% error)', color='red')\n",
    "plt.plot(xgb_accuracy_plot, label='XGBoost Accuracy (1% error)', color='green')\n",
    "plt.plot(cnn_accuracy_plot, label='CNN Accuracy (1% error)', color='orange')\n",
    "plt.axhline(y=1, color='blue', linestyle='--', label='Within 1% Error')\n",
    "plt.title('Prediction Accuracy (1 = within 1% error, 0 = outside 1% error)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
