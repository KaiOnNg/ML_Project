{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlfinlab as mlfin\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import pandas_profiling as pf\n",
    "import numpy as np\n",
    "from yahoo_finance import Share\n",
    "from datetime import datetime, timedelta, date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy import stats\n",
    "import pywt\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator, WilliamsRIndicator, ROCIndicator\n",
    "from ta.trend import MACD, ADXIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from xgboost import XGBClassifier, plot_importance, plot_tree\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.validators.scatter.marker import SymbolValidator\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, HBox, Label\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "sns.set_context('notebook', rc={\"axes.titlesize\":14,\"axes.labelsize\":13})\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "bbva = ['#004481','#2DCCCD', '#D8BE75','#1973B8', '#5BBEFF', '#F7893B', '#02A5A5', '#48AE64', '#F8CD51', '#F78BE8'];\n",
    "sns.set_palette(bbva);\n",
    "sns.color_palette(bbva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data='countries', data_type='News_Social', asset_code_id='US'):\n",
    "    \"\"\"\n",
    "    This function loads the sentiment indicators downloaded from Thomsom Reuters, \n",
    "    i.e. the MarketPsych Sentiment Indicators related to countries, companies and currencies.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: str\n",
    "        One of 'countries', 'companies', 'currencies'.\n",
    "    data_type: str\n",
    "        Whether to filter info by data source. One of 'News', 'Social', 'News_Social'.\n",
    "    asset_code_id: str\n",
    "        Code of the asset / data. One of 'US', 'US500', 'USD'.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    Pandas dataframe.\n",
    "    \"\"\"\n",
    "    dirs = os.listdir('./')\n",
    "    if data == 'countries':\n",
    "        dim, file = 'COU', 'COU_CARGA_INICIAL.csv'\n",
    "        sufix = '_USA'\n",
    "    elif data == 'currencies':\n",
    "        dim, file = 'CUR', 'CUR_CARGA_INICIAL.csv'\n",
    "        sufix = '_USD'\n",
    "    elif data == 'companies':\n",
    "        dim, file = 'CMPNY', 'CMPNY_GRP_CARGA_INICIAL.csv'\n",
    "        sufix = '_US500'\n",
    "        \n",
    "    dirs = [dire_x for dire_x in os.listdir('./') if dim in dire_x]\n",
    "    dataset = pd.read_csv(file, sep=';')\n",
    "    dataset = dataset[dataset.date <= '2020-03-30']\n",
    "    \n",
    "    for dire in dirs:\n",
    "        if (dire != file):\n",
    "            new_month = pd.read_csv(dire, delimiter='\\t')\n",
    "            if len(new_month.columns) == 1:\n",
    "                new_month = pd.read_csv(dire, delimiter=';')\n",
    "            if 'date' not in new_month.columns:\n",
    "                new_month['date'] = new_month.id.apply(lambda x: x[3:13])\n",
    "            if 'asset_code_id' not in new_month.columns:\n",
    "                new_month['asset_code_id'] = new_month.assetCode\n",
    "            if 'data_type' not in new_month.columns:\n",
    "                new_month['data_type'] = new_month.dataType\n",
    "            if 'id_refinitiv' not in new_month.columns:\n",
    "                new_month['id_refinitiv'] = new_month.id\n",
    "            if 'system_version' not in new_month.columns:\n",
    "                new_month['system_version'] = new_month.systemVersion\n",
    "            if 'date_audit_laod' not in new_month.columns:\n",
    "                new_month['date_audit_laod'] = 'NA'\n",
    "            if 'process_audit_load' not in new_month.columns:\n",
    "                new_month['process_audit_load'] = 'NA'\n",
    "            new_month = new_month[dataset.columns]\n",
    "            dataset = pd.concat([dataset, new_month], ignore_index=True)\n",
    "\n",
    "    dataset = dataset[(dataset.data_type == data_type) & \n",
    "                       (dataset.asset_code_id == asset_code_id) &\n",
    "                       (dataset.date >= '2000-01-01')].sort_values(by='date')\n",
    "    \n",
    "    dataset['Date'] = pd.to_datetime(dataset.date)\n",
    "    dataset.set_index('Date', inplace=True)\n",
    "    dataset.drop(['date', 'asset_code_id', 'data_type', 'id_refinitiv',\n",
    "                  'system_version', 'date_audit_laod', 'process_audit_load'], axis=1, inplace=True)\n",
    "    dataset.columns = [col + sufix for col in dataset.columns]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries info\n",
    "countries = load_data(data='countries', data_type='News_Social', asset_code_id='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currencies info\n",
    "currencies = load_data(data='currencies', data_type='News_Social', asset_code_id='USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currencies.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = load_data(data='companies', data_type='News_Social', asset_code_id='MPTRXUS500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on we will adapt dates to the data loaded before\n",
    "start_date = datetime(1999, 11, 30)\n",
    "end_date = datetime(2020, 8, 31)\n",
    "\n",
    "# SP500 Yahoo Finance\n",
    "sp500_yahoo = pdr.get_data_yahoo(symbols='^GSPC', start=start_date, end=end_date)\n",
    "sp500_yahoo = sp500_yahoo.asfreq('D', method=None) # generating extra days so that we don't have date jumps\n",
    "display(sp500_yahoo.head())\n",
    "\n",
    "# Adding return and volatility just for plotting (as this has to be calculated separately in train and test)\n",
    "original_columns = list(sp500_yahoo.columns)\n",
    "original_columns.remove('Volume') # unrealiable volume data. It will not be used\n",
    "sp500_yahoo['Daily Return'] = sp500_yahoo['Adj Close'].pct_change(periods=1)*100\n",
    "sp500_yahoo['Daily Volatility'] = sp500_yahoo['Daily Return'].ewm(span=30).std() # exponential moving std\n",
    "sp500_yahoo['Daily Expected Return'] = sp500_yahoo['Daily Return'].ewm(span=30).mean()\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "weekend = [6, 7]\n",
    "weekdays = []\n",
    "for dt in daterange(start_date, end_date):\n",
    "    if dt.isoweekday() not in weekend:\n",
    "        weekdays.append(dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "# We'll take only weekdays and we'll delete weekends (as markets are closed during these days)\n",
    "sp500_yahoo = sp500_yahoo[sp500_yahoo.index.isin(weekdays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting price, volatility and return\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Daily Volatility [%]')\n",
    "ax1.plot(sp500_yahoo['Daily Volatility'], label='Daily Volatility', color='lightgrey')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.set_ylabel('S&P 500 Close Price')  # we already handled the x-label with ax1\n",
    "ax2.plot(sp500_yahoo['Close'], color=bbva[0], label='S&P 500 Close Price')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(sp500_yahoo['Daily Return'], color=bbva[0], label='Daily Return');\n",
    "ax.plot(sp500_yahoo['Daily Expected Return'] + sp500_yahoo['Daily Volatility'], color='lightgrey')\n",
    "ax.plot(sp500_yahoo['Daily Expected Return'] - sp500_yahoo['Daily Volatility'], color='lightgrey', label='Volatility Bands')\n",
    "ax.plot(sp500_yahoo['Daily Expected Return'], color=bbva[1], label='Daily Expected Return')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylabel('Daily Return [%]');\n",
    "ax.set_xlabel('Date');\n",
    "#plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating events with cusum filter (just for plotting)\n",
    "cusum_events = mlfin.filters.cusum_filter(sp500_yahoo['Adj Close'], \n",
    "                                          threshold=0.1) #threshold abs(change)\n",
    "\n",
    "# interactive plot\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#configure_plotly_browser_state()\n",
    "\n",
    "# creating widgets\n",
    "dependent=widgets.Select(options=['Close', 'Daily Return', 'Daily Volatility'],\n",
    "                         value='Close', description='View', disabled=False)\n",
    "dataframe=widgets.RadioButtons(options=['Companies', 'Countries', 'Currencies'], \n",
    "                               value='Companies', description='Indices', disabled=False)\n",
    "sentiment1=widgets.Dropdown(options=companies.columns,\n",
    "                            value='sentiment_US500', description='Comp. Value', disabled=False)\n",
    "sentiment2=widgets.Dropdown(options=countries.columns,\n",
    "                            value='stockIndexSentiment_USA', description='Count. Value', disabled=False)\n",
    "sentiment3=widgets.Dropdown(options=currencies.columns,\n",
    "                            value='sentiment_USD', description='Curr. Value',\n",
    "                            disabled=False, layout={'positioning': 'right'})\n",
    "\n",
    "# setting the ui for our widgets\n",
    "ui = widgets.HBox([dependent, dataframe, widgets.VBox([sentiment1, sentiment2, sentiment3])])\n",
    "\n",
    "#@interact\n",
    "def plot_sentiment_index(dependent, dataframe, sentiment1, sentiment2, sentiment3):\n",
    "    \n",
    "    if dataframe == 'Companies':\n",
    "        sentiment = sentiment1\n",
    "        df = companies\n",
    "    elif dataframe == 'Countries':\n",
    "        sentiment = sentiment2\n",
    "        df = countries\n",
    "    elif dataframe == 'Currencies':\n",
    "        sentiment = sentiment3\n",
    "        df = currencies\n",
    "    \n",
    "    figura = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    figura.add_trace(go.Scatter(y=sp500_yahoo[dependent].fillna('ffill'),\n",
    "                                x=sp500_yahoo.index,\n",
    "                                mode='lines',\n",
    "                                name='S&P 500 '+ dependent),\n",
    "                     secondary_y=True,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=df[sentiment],\n",
    "                   x=df.index,\n",
    "                   mode='lines',\n",
    "                   name=sentiment + ' Index',\n",
    "                   visible='legendonly'),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=df[sentiment].ewm(span=365).mean(),\n",
    "                   x=df.index,\n",
    "                   mode='lines',\n",
    "                   name='EWMA 1y ' + sentiment[:5] + '. Index'),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=df[sentiment].ewm(span=180).mean(),\n",
    "                   x=df.index,\n",
    "                   mode='lines',\n",
    "                   name='EWMA 6m ' + sentiment[:5] + '. Index'),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=df[sentiment].ewm(span=90).mean(),\n",
    "                   x=df.index,\n",
    "                   mode='lines',\n",
    "                   name='EWMA 3m ' + sentiment[:5] + '. Index'),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=df[sentiment].ewm(span=30).mean(),\n",
    "                   x=df.index,\n",
    "                   mode='lines',\n",
    "                   name='EWMA 1m ' + sentiment[:5] + '. Index',\n",
    "                   visible='legendonly'),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(go.Scatter(y=sp500_yahoo['Adj Close'][cusum_events],\n",
    "                                x=cusum_events,\n",
    "                                mode='markers',\n",
    "                                name='S&P 500 Index CUSUM Events'),\n",
    "                     secondary_y=True,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=[0],\n",
    "                   x=['2001-09-11'],\n",
    "                   mode='markers',\n",
    "                   name='Sept 11 Attacks',\n",
    "                   marker=dict(size=15),\n",
    "                   marker_symbol=17),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=[0],\n",
    "                   x=['2002-10-09'],\n",
    "                   mode='markers',\n",
    "                   name='Dot-Com Bubble Burst',\n",
    "                   marker=dict(size=15),\n",
    "                   marker_symbol=17),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=[0],\n",
    "                   x=['2008-09-15'],\n",
    "                   mode='markers',\n",
    "                   name='Lehman Brothers Collapse',\n",
    "                   marker=dict(size=15),\n",
    "                   marker_symbol=17),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=[0],\n",
    "                   x=['2018-12-22'],\n",
    "                   mode='markers',\n",
    "                   name='U.S. Federal Government Shutdown',\n",
    "                   marker=dict(size=15),\n",
    "                   marker_symbol=17),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=[0],\n",
    "                   x=['2020-01-20'],\n",
    "                   mode='markers',\n",
    "                   name='1st COVID-19 Case USA',\n",
    "                   marker=dict(size=15),\n",
    "                   marker_symbol=17),\n",
    "                   secondary_y=False,)\n",
    "\n",
    "    figura.update_layout(\n",
    "        title_text='S&P 500 Index vs Sentiment Indices | Indicator: {}'.format(sentiment),\n",
    "        colorway = bbva)\n",
    "\n",
    "    figura.update_xaxes(rangeslider_visible=True)\n",
    "    figura.update_yaxes(title_text=\"<b>Sentiment Index</b>\", secondary_y=False)\n",
    "    figura.update_yaxes(title_text=\"<b>S&P 500 Close Price</b>\", secondary_y=True)\n",
    "\n",
    "    figura.update_xaxes(\n",
    "        rangeslider_visible=True,\n",
    "        rangeselector=dict(\n",
    "            dict(font = dict(color = \"black\")),\n",
    "            buttons=list([\n",
    "                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(count=3, label=\"3y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\"),\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    figura.update_layout(template='simple_white', hovermode='x')\n",
    "    iplot(figura)\n",
    "    \n",
    "out = widgets.interactive_output(plot_sentiment_index, {'dependent': dependent, 'dataframe': dataframe, \n",
    "                                                        'sentiment1': sentiment1, 'sentiment2': sentiment2,\n",
    "                                                        'sentiment3': sentiment3})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only take the original columns (remember that we created three new ones just for plotting)\n",
    "sp500_yahoo = sp500_yahoo[original_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the datasets\n",
    "sentiments = companies.merge(currencies.merge(countries, \n",
    "                                              left_index=True, right_index=True), left_index=True, right_index=True)\n",
    "sentiments.drop_duplicates(inplace=True)\n",
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll calculate a weighted average on Mondays adding info from the weekend\n",
    "mondays_weekends = []\n",
    "for dt in daterange(start_date, end_date):\n",
    "    if dt.isoweekday() in [1, 6, 7]:\n",
    "        mondays_weekends.append(dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "senti_mon = sentiments.reindex(pd.DatetimeIndex(mondays_weekends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First day was Saturday (this will be useful for allocating weights)\n",
    "senti_mon.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation:\n",
    "for col in senti_mon.columns:\n",
    "    senti_mon[col] = \\\n",
    "        senti_mon[col].rolling(3).apply(lambda x: np.average(x, weights=[0.12, 0.22, 0.66])) \n",
    "        # damos mayor peso a los lunes (mon 2/3, sun 2/9, sat 1/9)\n",
    "        \n",
    "# Substituting indices corresponding to senti_mon\n",
    "sentiments[sentiments.index.isin(mondays_weekends)] = senti_mon\n",
    "\n",
    "del senti_mon\n",
    "\n",
    "# Deleting weekends\n",
    "sentiments = sentiments[sentiments.index.isin(weekdays)]\n",
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting train and test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "\n",
    "train_start, train_end = '2001-08-31', '2016-08-31'\n",
    "test_start, test_end = '2016-09-01', '2020-08-31'\n",
    "\n",
    "sentiments_test = sentiments[test_start:test_end]\n",
    "sentiments_train = sentiments[train_start:train_end]\n",
    "sp500_test = sp500_yahoo[test_start:test_end]\n",
    "sp500_train = sp500_yahoo[train_start:train_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test ratios\n",
    "print(\"{0:.0%}\".format(len(sp500_train)/len(sp500_yahoo[train_start:])))\n",
    "print(\"{0:.0%}\".format(len(sp500_test)/len(sp500_yahoo[train_start:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll plot the rate of variables with missing values over time\n",
    "sentiments_train['year'] = sentiments_train.index.year\n",
    "sentiments_train['missing'] = sentiments_train.isnull().sum(axis=1)\n",
    "\n",
    "((1-(sentiments_train.groupby('year').count().min(axis=1)/\n",
    "    sentiments_train.groupby('year').count()['bondBuzz_USA']))*100).plot(legend=False, marker='o');\n",
    "((1-(sentiments_train.groupby('year').count().mean(axis=1)/\n",
    "    sentiments_train.groupby('year').count()['bondBuzz_USA']))*100).plot(legend=False, marker='o');\n",
    "plt.title('Max. and mean missing-value rates per year');\n",
    "plt.ylabel('Missing rate [%]');\n",
    "plt.xlabel('Date');\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sentiments_train.groupby('year').max().index, \n",
    "         list(sentiments_train.groupby('year').max()['missing']), marker='o');\n",
    "plt.plot(sentiments_train.groupby('year').mean().index, \n",
    "         list(sentiments_train.groupby('year').mean()['missing']), marker='o');\n",
    "plt.title('Max. and mean number of variables with missing values per year');\n",
    "plt.ylabel('Number of variables');\n",
    "plt.xlabel('Date');\n",
    "\n",
    "sentiments_train.drop(['year', 'missing'], axis=1, inplace=True)\n",
    "\n",
    "# y axis represents the percentage of variables with missing values for every year \n",
    "# disclaimer: these are not missing value rates, see section Data Loading for checking those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping cols with a missing rate larger than 20%\n",
    "missing_rate = (sentiments_train.isnull().sum() / len(sentiments_train))*100\n",
    "drop_cols = [col for col in missing_rate.index if missing_rate[col] >= 20]\n",
    "print(drop_cols)\n",
    "sentiments_train.drop(drop_cols, axis=1, inplace=True)\n",
    "sentiments_test.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values for the rest of variables\n",
    "# We'll perform a forward filling since we're dealing with news\n",
    "\n",
    "sentiments_train.fillna(method='ffill', inplace=True) \n",
    "sentiments_test.fillna(method='ffill', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msno.matrix(sp500_yahoo[['Close']])\n",
    "\n",
    "# We'll interpolate over the days were there is no data (be it due to the closing of markets in festive days or\n",
    "# just because of an absence of data due to system errors)\n",
    "\n",
    "sp500_train.interpolate(method='spline', order=3, limit_direction='forward', axis=0, inplace=True) \n",
    "sp500_test.interpolate(method='spline', order=3, limit_direction='forward', axis=0, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll apply yeo johnson to balance dispersion amongst varaibles\n",
    "# in this chunk we'll only apply it to the sentiments dataset. Later on we'll apply it to the financial variables\n",
    "#example\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(211)\n",
    "x = currencies.buzz_USD\n",
    "prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_title('Probplot against normal distribution')\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "xt, l = stats.yeojohnson(x)\n",
    "prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
    "ax2.set_title('Probplot after Box-Cox transformation')\n",
    "plt.show()\n",
    "print('Lambda: ', l)\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].hist(x, bins=30);\n",
    "ax[1].hist(xt, bins=30);\n",
    "ax[0].set_title('Original and transformed variable');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same lambdas for test dataset\n",
    "\n",
    "def apply_transformation(data_train, data_test, transformation):\n",
    "    \"\"\"\n",
    "    Applies dispersion and scale transformations on data split in train and test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_train: pandas dataframe\n",
    "        Train set.\n",
    "    data_test: pandas dataframe\n",
    "        Test set.\n",
    "    transformation: str\n",
    "        One of 'dispersion', 'scale' or 'dispersion_and_scale'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple with train set and test set.\n",
    "    \"\"\" \n",
    "    \n",
    "    data_train = data_train.dropna()\n",
    "    data_test = data_test.dropna()\n",
    "    \n",
    "    index_train = data_train.index\n",
    "    index_test = data_test.index\n",
    "    \n",
    "    if transformation == 'dispersion':\n",
    "        for col in data_train.columns:\n",
    "            data_train[col], fitted_lambda = stats.yeojohnson(data_train[col])\n",
    "            data_test[col] = stats.yeojohnson(data_test[col], fitted_lambda)\n",
    "            \n",
    "    elif transformation == 'scale':\n",
    "        scaler = StandardScaler().fit(data_train)\n",
    "        std_train = scaler.transform(data_train)\n",
    "        std_test = scaler.transform(data_test)\n",
    "        data_train = pd.DataFrame(std_train, columns=data_train.columns)\n",
    "        data_test = pd.DataFrame(std_test, columns=data_test.columns)\n",
    "        data_train['Date'] = index_train\n",
    "        data_test['Date'] = index_test\n",
    "        data_train.set_index('Date', inplace=True)\n",
    "        data_test.set_index('Date', inplace=True)\n",
    "        \n",
    "    elif transformation == 'dispersion_and_scale':\n",
    "        for col in data_train.columns:\n",
    "            data_train[col], fitted_lambda = stats.yeojohnson(data_train[col])\n",
    "            data_test[col] = stats.yeojohnson(data_test[col], fitted_lambda)\n",
    "        scaler = StandardScaler().fit(data_train)\n",
    "        std_train = scaler.transform(data_train)\n",
    "        std_test = scaler.transform(data_test)\n",
    "        data_train = pd.DataFrame(std_train, columns=data_train.columns)\n",
    "        data_test = pd.DataFrame(std_test, columns=data_test.columns)\n",
    "        data_train['Date'] = index_train\n",
    "        data_test['Date'] = index_test\n",
    "        data_train.set_index('Date', inplace=True)\n",
    "        data_test.set_index('Date', inplace=True)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking linear correlation before transformation\n",
    "sentiments_train.optimism_US500.ewm(90).mean().corr(sp500_yahoo.Close, method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also apply standardization to even scales. This will make distributions more comparable\n",
    "sentiments_train, sentiments_test = apply_transformation(sentiments_train, sentiments_test, 'dispersion_and_scale')\n",
    "\n",
    "display(sentiments_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking linear correlation after transformation\n",
    "sentiments_train.optimism_US500.ewm(90).mean().corr(sp500_yahoo.Close, method='pearson') \n",
    "# correlation has improved a little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing fast fourier transform\n",
    "def filter_signal(signal, threshold=1e8):\n",
    "    \"\"\"\n",
    "    Performs a Fast Fourier Transform over a signal and returns filtered data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal: numpy array\n",
    "    threshold: double\n",
    "    \"\"\"\n",
    "    fourier = np.fft.rfft(signal)\n",
    "    frequencies = np.fft.rfftfreq(signal.size, d=1.)\n",
    "    fourier[frequencies > threshold] = 0\n",
    "    return np.fft.irfft(fourier)\n",
    "\n",
    "span = 500\n",
    "signal = np.array(sentiments_train.sentiment_US500[1:])\n",
    "threshold = 0.1\n",
    "filtered = filter_signal(signal, threshold=threshold)\n",
    "plt.plot(signal[-span:], label='Raw')\n",
    "\n",
    "plt.plot(filtered[-span:], label='Filtered')\n",
    "plt.plot(np.array(pd.Series(signal).ewm(22).mean())[-span:], label='22-day EWM')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"FFT Denoising with threshold = {} cycles per day\".format(threshold), size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Sentiment Index')\n",
    "\n",
    "ax1.plot(sentiments['sentiment_US500'][train_start:train_end][:300], label='Raw Sentiment US500', color=bbva[2])\n",
    "ax1.plot(sentiments['sentiment_US500'][train_start:train_end].ewm(22).mean()[:300], label='EWMA Sentiment US500', \n",
    "         color=bbva[1])\n",
    "ax1.legend(loc='lower left')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.set_ylabel('S&P 500 Close Price')  # we already handled the x-label with ax1\n",
    "ax2.plot(sp500_train['Adj Close'][:300], color=bbva[0], label='S&P 500 Close Price')\n",
    "ax2.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for selecting smoothing technique\n",
    "def select_smoothing(data, technique, span0=22):\n",
    "    \"\"\"\n",
    "    Performs a smoothing (EWMA) or filtering (FFT) technique over data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas dataframe column\n",
    "    technique: str\n",
    "        One of 'fft' for Fast Fourier Transform or 'ewma' for Exponentially Weighted Moving Average.\n",
    "        Calls function filter_signal() for FFT.\n",
    "    span0: int\n",
    "        Decay in terms of span.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas dataframe column.\n",
    "    \"\"\"\n",
    "    if technique == 'fft':\n",
    "        filtered = list(filter_signal(data, threshold=threshold))\n",
    "        filtered.append(0)\n",
    "    elif technique == 'ewma':\n",
    "        filtered = data.ewm(span=span0).mean()\n",
    "    return filtered\n",
    "\n",
    "# we'll be using the ewma\n",
    "for col in sentiments_train.columns:\n",
    "    sentiments_train[col] = select_smoothing(sentiments_train[col], 'ewma')\n",
    "    sentiments_test[col] = select_smoothing(sentiments_test[col], 'ewma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll add some technical indicators used in trading for evaluating momemtum and trends\n",
    "def add_technical_indicators(data):\n",
    "    \"\"\"\n",
    "    Adds technical indicators widely used by traders when checking for bearish or bullish signals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "   \n",
    "    data['ROC'] = ROCIndicator(data['Adj Close'], 10).roc()\n",
    "    data['RSI'] = RSIIndicator(data['Adj Close'], 10).rsi()\n",
    "    data['Stoch'] = StochasticOscillator(high=data['High'], \n",
    "                                         low=data['Low'], \n",
    "                                         close=data['Close'], \n",
    "                                         n=10).stoch()\n",
    "    data['Williams'] = WilliamsRIndicator(high=data['High'], \n",
    "                                          low=data['Low'], \n",
    "                                          close=data['Close'],  \n",
    "                                          lbp=10).wr()\n",
    "    data['MACD'] = MACD(data['Close'], \n",
    "                        n_slow = 22, \n",
    "                        n_fast = 8, \n",
    "                        n_sign = 5).macd()\n",
    "    data['ADX'] = ADXIndicator(high=data['High'], \n",
    "                               low=data['Low'], \n",
    "                               close=data['Close'], n=10).adx()\n",
    "    \n",
    "    data['Close/Open'] = [1 if data.Close[x] > data.Open[x] else 0\n",
    "                          for x in range(0, len(data))]\n",
    "    \n",
    "    data['Daily Return'] = data['Adj Close'].pct_change(periods=1)*100\n",
    "    data['Daily Volatility'] = data['Daily Return'].ewm(span=22).std() # exponential moving std\n",
    "    \n",
    "    ### only compute this cross if it won't later be the primary model!!\n",
    "    fast_window = 20\n",
    "    slow_window = 60\n",
    "    col = 'Adj Close'\n",
    "    data['Fast EWMA {}'.format(col)] = data[col] # already averaged\n",
    "    data['Slow EWMA {}'.format(col)] = data[col].ewm(slow_window).mean()\n",
    "\n",
    "    # Compute sides\n",
    "    data['sp_cross_{}'.format(col)] = np.nan\n",
    "\n",
    "    long_signals = data['Fast EWMA {}'.format(col)] >= data['Slow EWMA {}'.format(col)]\n",
    "    short_signals = data['Fast EWMA {}'.format(col)] < data['Slow EWMA {}'.format(col)]\n",
    "    data.loc[long_signals, 'sp_cross_{}'.format(col)]  = 1\n",
    "    data.loc[short_signals, 'sp_cross_{}'.format(col)]  = -1\n",
    "\n",
    "    # Lagging our trading signals by one day\n",
    "\n",
    "    data.drop(['Fast EWMA {}'.format(col), 'Slow EWMA {}'.format(col)], axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "sp500_train = add_technical_indicators(sp500_train)\n",
    "sp500_test = add_technical_indicators(sp500_test)\n",
    "\n",
    "sp500_train.Close[-100:].plot(legend=True);\n",
    "plt.figure()\n",
    "sp500_train.MACD[-100:].plot(legend=True);\n",
    "sp500_train.ADX[-100:].plot(legend=True);\n",
    "sp500_train.RSI[-100:].plot(legend=True);\n",
    "sp500_train.Stoch[-100:].plot(legend=True);\n",
    "sp500_train.Williams[-100:].plot(legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_train.drop(['Close'], axis=1, inplace=True)\n",
    "sp500_test.drop(['Close'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're shifting forward the financial variables (which are related to price) since news from the\n",
    "# previous day have to be used for predicting next day's prices\n",
    "sp500_train = sp500_train.shift(1) \n",
    "sp500_test = sp500_test.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before continuing, we'll concat the financial and the sentiment datasets\n",
    "X_train = pd.concat([sp500_train, sentiments_train], axis=1)\n",
    "X_test = pd.concat([sp500_test, sentiments_test], axis=1)\n",
    "X_train.dropna(inplace=True) # there are na values at the beginning, for the newly created variables (technical indicators)\n",
    "X_test.dropna(inplace=True)\n",
    "\n",
    "# Now we'll create y_train and y_test, our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll add these ewmas as predictor or explanatory variables\n",
    "def add_crossing_ewmas(data, fast_window, slow_window):\n",
    "    \"\"\"\n",
    "    Adds two crossing exponentially weighted moving averages. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas dataframe\n",
    "    fast_window: int\n",
    "        Fast decay in terms of span.\n",
    "    slow_window: int\n",
    "        Slow decay in terms of span.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    for col in ['sentiment_US500', 'stockIndexSentiment_USA']:\n",
    "\n",
    "        data['Fast EWMA {}'.format(col)] = data[col] # already averaged\n",
    "        data['Slow EWMA {}'.format(col)] = data[col].ewm(slow_window).mean()\n",
    "\n",
    "        # Compute sides\n",
    "        data['cross_{}'.format(col)] = np.nan\n",
    "\n",
    "        long_signals = data['Fast EWMA {}'.format(col)] >= data['Slow EWMA {}'.format(col)]\n",
    "        short_signals = data['Fast EWMA {}'.format(col)] < data['Slow EWMA {}'.format(col)]\n",
    "        data.loc[long_signals, 'cross_{}'.format(col)]  = 1\n",
    "        data.loc[short_signals, 'cross_{}'.format(col)]  = -1\n",
    "\n",
    "        \n",
    "\n",
    "        data.drop(['Fast EWMA {}'.format(col), 'Slow EWMA {}'.format(col)], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "X_train = add_crossing_ewmas(X_train, 10, 60)\n",
    "X_test = add_crossing_ewmas(X_test, 10, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overriding function get_bins\n",
    "# we want this to return the sign of the return when the vertical barrier is touched first\n",
    "# instead of what it's currently implemented (0 if vertical barrier is touched first)\n",
    "def get_bins(triple_barrier_events, close):\n",
    "    \"\"\"\n",
    "    Advances in Financial Machine Learning, Snippet 3.7, page 51.\n",
    "\n",
    "    Labeling for Side & Size with Meta Labels\n",
    "\n",
    "    Compute event's outcome (including side information, if provided).\n",
    "    events is a DataFrame where:\n",
    "\n",
    "    Now the possible values for labels in out['bin'] are {0,1}, whether to take the bet or pass,\n",
    "    a purely binary prediction. The previous feasible values were {âˆ’1,0,1}.\n",
    "    The ML algorithm will be trained to decide if it's 1, and we can use the probability of this secondary prediction\n",
    "    to derive the size of the bet, where the side (sign) of the position has been set by the primary model.\n",
    "\n",
    "    :param triple_barrier_events: (pd.DataFrame)\n",
    "                -events.index is event's starttime\n",
    "                -events['t1'] is event's endtime\n",
    "                -events['trgt'] is event's target\n",
    "                -events['side'] (optional) implies the algo's position side\n",
    "                Case 1: ('side' not in events): bin in (-1,1) <-label by price action\n",
    "                Case 2: ('side' in events): bin in (0,1) <-label by pnl (meta-labeling)\n",
    "    :param close: (pd.Series) Close prices\n",
    "    :return: (pd.DataFrame) Meta-labeled events\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Align prices with their respective events\n",
    "    events_ = triple_barrier_events.dropna(subset=['t1'])\n",
    "    all_dates = events_.index.union(other=events_['t1'].array).drop_duplicates()\n",
    "    prices = close.reindex(all_dates, method='bfill')\n",
    "\n",
    "    # 2) Create out DataFrame\n",
    "    out_df = pd.DataFrame(index=events_.index)\n",
    "    # Need to take the log returns, else your results will be skewed for short positions\n",
    "   \n",
    "    out_df['ret'] = prices.loc[events_['t1'].values].values / prices.loc[events_.index] - 1\n",
    "    out_df['trgt'] = events_['trgt']\n",
    "\n",
    "    # Meta labeling: Events that were correct will have pos returns\n",
    "    if 'side' in events_:\n",
    "        out_df['ret'] = out_df['ret'] * events_['side']  # meta-labeling\n",
    "\n",
    "    # Added code: label 0 when vertical barrier reached\n",
    "    #-------------------we change this step, as we want the outcome to be the sign of the return\n",
    "   \n",
    "    out_df['bin'] = np.sign(out_df['ret'])\n",
    "\n",
    "    # Meta labeling: label incorrect events with a 0\n",
    "    if 'side' in events_:\n",
    "        out_df.loc[out_df['ret'] <= 0, 'bin'] = 0\n",
    "\n",
    "    # Transform the log returns back to normal returns.\n",
    "   \n",
    "    # Add the side to the output. This is useful for when a meta label model must be fit\n",
    "    tb_cols = triple_barrier_events.columns\n",
    "    if 'side' in tb_cols:\n",
    "        out_df['side'] = triple_barrier_events['side']\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll apply the labeling\n",
    "def apply_trading_labeling(data, \n",
    "                           compute_side=False, \n",
    "                           horizon=14,\n",
    "                           pt_sl=[1, 2], # multipliers for daily vol (contribution to horizontal barriers)\n",
    "                           min_ret=0.005,\n",
    "                           primary_model=False):\n",
    "    \"\"\"\n",
    "    Applies selected trading strategy with the given parameters. Labeling uses Triple Barrier Method.\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    dataframe: pandas dataframe \n",
    "        Data to use.\n",
    "    compute_side: boolean\n",
    "        Whether to use a primary model that tells the side (buy or sell).\n",
    "        When True, a trend-following strategy will be applied as primary model.\n",
    "        Default is False.\n",
    "    horizon: int\n",
    "        Prediction horizon in natural days.\n",
    "    pt_sl: list\n",
    "        Profit taking and stop loss multipliers to the volatility. Width of the TBM box.\n",
    "    min_ret: float\n",
    "        Minimum target return to run the search for triple barriers.\n",
    "    primary_model: boolean\n",
    "        Whether a primary model computed by the user has already decided the side.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Labels dataframe and triple-barrier events dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ####--------------------- Primary models ------------------------####\n",
    "    if compute_side is True:\n",
    "        # compute exponentially moving averages\n",
    "        fast_window = 20\n",
    "        slow_window = 90 # optimize the span for fast and slow averages\n",
    "\n",
    "        data['Fast EWMA'] = data['Adj Close'].ewm(fast_window).mean()\n",
    "        data['Slow EWMA'] = data['Adj Close'].ewm(slow_window).mean()\n",
    "    \n",
    "        # Compute sides\n",
    "        data['Side'] = np.nan\n",
    "\n",
    "        long_signals = data['Fast EWMA'] >= data['Slow EWMA']\n",
    "        short_signals = data['Fast EWMA'] < data['Slow EWMA']\n",
    "        data.loc[long_signals, 'Side'] = 1\n",
    "        data.loc[short_signals, 'Side'] = -1\n",
    "\n",
    "        # Lagging our trading signals by one day\n",
    "        data[['Fast EWMA', 'Slow EWMA']] = data[['Fast EWMA', 'Slow EWMA']].shift(1)\n",
    "\n",
    "        data[['Fast EWMA', 'Slow EWMA']].plot();\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    ####--------------------- CUSUM filters ------------------------####\n",
    "    # Apply Symmetric CUSUM Filter and get timestamps for events\n",
    "    cusum_events = mlfin.filters.cusum_filter(data['Adj Close'],\n",
    "                                              threshold=data['Daily Volatility']/100)\n",
    "\n",
    "    ####--------------------- Vertical barriers ------------------------####\n",
    "    # Compute vertical barrier\n",
    "    vertical_barriers = mlfin.labeling.add_vertical_barrier(t_events=cusum_events,\n",
    "                                                            close=data['Adj Close'],\n",
    "                                                            num_days=horizon) # this is the length of the tbm box\n",
    "    \n",
    "    ####--------------------- Triple barriers ------------------------####\n",
    "    # Computing triple barriers\n",
    "    if (compute_side is True) | (primary_model is True):\n",
    "        triple_barrier_events = mlfin.labeling.get_events(close=data['Adj Close'],\n",
    "                                                       t_events=cusum_events,\n",
    "                                                       pt_sl=pt_sl, # profit taking and stop loss multiples\n",
    "                                                       target=data['Daily Volatility']/100, # values in conjunction with pt_sl for width of barrier\n",
    "                                                       min_ret=min_ret,\n",
    "                                                       num_threads=3, # num of parallel tasks\n",
    "                                                       vertical_barrier_times=vertical_barriers,\n",
    "                                                       side_prediction=data.Side)\n",
    "        ####--------------------- Meta-labels ------------------------####\n",
    "        # now we compute the meta-labelling\n",
    "        meta_labeled_events = get_bins(triple_barrier_events, data['Adj Close'])\n",
    "    \n",
    "    else:\n",
    "        triple_barrier_events = mlfin.labeling.get_events(close=data['Adj Close'],\n",
    "                                                       t_events=cusum_events,\n",
    "                                                       pt_sl=pt_sl, # profit taking and stop loss multiples\n",
    "                                                       target=data['Daily Volatility']/100, # values in conjunction with pt_sl for width of barrier\n",
    "                                                       min_ret=min_ret,\n",
    "                                                       num_threads=3, # num of parallel tasks\n",
    "                                                       vertical_barrier_times=vertical_barriers,\n",
    "                                                       side_prediction=None)\n",
    "\n",
    "        ####--------------------- Side ------------------------####\n",
    "        # now we compute the side \n",
    "        # function that does meta-labeling returns side if no side prediction comes first\n",
    "        meta_labeled_events = get_bins(triple_barrier_events, data['Adj Close'])\n",
    "        meta_labeled_events['side'] = meta_labeled_events['bin']\n",
    "        meta_labeled_events['bin'] = 1\n",
    "\n",
    "    return meta_labeled_events, triple_barrier_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing labels for side (+1, -1) \n",
    "# side will tell the sign of the bet\n",
    "train_labels, tbm_train = apply_trading_labeling(X_train, \n",
    "                                     primary_model=False, \n",
    "                                     compute_side=False,\n",
    "                                     horizon=14,\n",
    "                                     pt_sl=[0, 0],\n",
    "                                     min_ret=0.005)\n",
    "test_labels, tbm_test = apply_trading_labeling(X_test, \n",
    "                                     primary_model=False, \n",
    "                                     compute_side=False,\n",
    "                                     horizon=14,\n",
    "                                     pt_sl=[0, 0],\n",
    "                                     min_ret=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_labels.head())\n",
    "display(tbm_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting labels to see outcome\n",
    "#configure_plotly_browser_state()\n",
    "@interact\n",
    "def plot_sentiment_index(data=widgets.RadioButtons(\n",
    "                            options=['Train', 'Test'],\n",
    "                            value='Test',\n",
    "                            # rows=10,\n",
    "                            description='Data',\n",
    "                            disabled=False)):\n",
    "\n",
    "    figura = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "    if data == 'Test':\n",
    "        dataset = X_test.merge(test_labels,\n",
    "                            left_index=True,\n",
    "                            right_index=True,\n",
    "                            how='left')\n",
    "        figura.add_trace(go.Scatter(y=X_test['Adj Close'],\n",
    "                                    x=X_test.index,\n",
    "                                    mode='lines',\n",
    "                                    name='S&P 500 Close Price'),\n",
    "                         secondary_y=False,)\n",
    "    else:\n",
    "        dataset = X_train.merge(train_labels,\n",
    "                            left_index=True,\n",
    "                            right_index=True,\n",
    "                            how='left')\n",
    "        figura.add_trace(go.Scatter(y=X_train['Adj Close'],\n",
    "                                    x=X_train.index,\n",
    "                                    mode='lines',\n",
    "                                    name='SP500 Close Price'),\n",
    "                         secondary_y=False,)\n",
    "    if ('Fast EWMA' in dataset.columns) and ('Slow EWMA' in dataset.columns):\n",
    "        figura.add_trace(go.Scatter(y=dataset['Fast EWMA'],\n",
    "                                    x=dataset.index,\n",
    "                                    mode='lines',\n",
    "                                    name='SP500 Fast EWMA'),\n",
    "                         secondary_y=False,)\n",
    "        figura.add_trace(go.Scatter(y=dataset['Slow EWMA'],\n",
    "                                    x=dataset.index,\n",
    "                                    mode='lines',\n",
    "                                    name='SP500 Slow EWMA'),\n",
    "                         secondary_y=False,)\n",
    "\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=dataset[(dataset.bin == 1) & (dataset.side == 1)]['Adj Close'],\n",
    "                   x=dataset[(dataset.bin == 1) & (dataset.side == 1)].index,\n",
    "                   mode='markers',\n",
    "                   name='Buy',\n",
    "                   marker=dict(size=8, color='#008000'),\n",
    "                   marker_symbol=5),\n",
    "                   secondary_y=False,)\n",
    "    figura.add_trace(\n",
    "        go.Scatter(y=dataset[(dataset.bin == 1) & (dataset.side == -1)]['Adj Close'],\n",
    "                   x=dataset[(dataset.bin == 1) & (dataset.side == -1)].index,\n",
    "                   mode='markers',\n",
    "                   name='Sell',\n",
    "                   marker=dict(size=8, color='#FF0000'),\n",
    "                   marker_symbol=6),\n",
    "                   secondary_y=False,)\n",
    "\n",
    "    figura.update_layout(\n",
    "        title_text='S&P 500 Index and labeled positions | {}'.format(data),\n",
    "        colorway = bbva)\n",
    "\n",
    "    figura.update_xaxes(rangeslider_visible=True)\n",
    "    figura.update_yaxes(title_text=\"<b>S&P 500 Close Price</b>\", secondary_y=False)\n",
    "\n",
    "    figura.update_xaxes(\n",
    "        rangeslider_visible=True,\n",
    "        rangeselector=dict(\n",
    "            dict(font = dict(color = \"black\")),\n",
    "            buttons=list([\n",
    "                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(count=3, label=\"3y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\"),\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    figura.update_layout(template='simple_white', hovermode='x')\n",
    "    iplot(figura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations per label\n",
    "train_labels['label'] = train_labels['bin'] * train_labels['side']\n",
    "print('Train:\\n', train_labels.groupby('label').count()[['ret']])\n",
    "\n",
    "test_labels['label'] = test_labels['bin'] * test_labels['side']\n",
    "print('\\nTest:\\n', test_labels.groupby('label').count()[['ret']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling price variables since we've already computed our sign labels\n",
    "X_train[sp500_train.columns], X_test[sp500_test.columns] = apply_transformation(X_train[sp500_train.columns],\n",
    "                                                                                X_test[sp500_test.columns],\n",
    "                                                                                'dispersion_and_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminating label 0 (0 would mean that pct change between days is null)\n",
    "train_labels = train_labels[train_labels.label != 0]\n",
    "test_labels = test_labels[test_labels.label != 0]\n",
    "\n",
    "# downsampling with events\n",
    "X_train = X_train.reindex(train_labels.index)\n",
    "X_test = X_test.reindex(test_labels.index)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying our model\n",
    "def apply_model(train_labels, test_labels, X_train_, X_test_, model, scoring, sfs=True):\n",
    "    \"\"\"\n",
    "    Applies model RF or XGB to data with Cross-Validation. It may perform variable selection on demand.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_labels: pandas dataframe\n",
    "        Dataframe containing train labels as returned by apply_trading_labeling.\n",
    "    test_labels: pandas dataframe\n",
    "        Dataframe containing test labels as returned by apply_trading_labeling.\n",
    "    X_train_: pandas dataframe\n",
    "        Training data.\n",
    "    X_test_: pandas dataframe\n",
    "        Test data.\n",
    "    model: str\n",
    "        One of 'RF' for Random Forest or 'XGB' for Extreme Gradient Boosting.\n",
    "    scoring: str\n",
    "        One of the admitted possibilities in sklearn's GridSearchCV .\n",
    "    sfs: boolean\n",
    "        Whether to perfrom Sequential Forward Selection with a simple RF for variable selection.\n",
    "        It will select from 10 to 30 total variables.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Best estimator from CV and list of selected variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test = test_labels.label\n",
    "    y_train = train_labels.label\n",
    "    \n",
    "    pos_ratio = round(train_labels[train_labels.label <= 0][['label']].count() / \n",
    "                      train_labels[train_labels.label > 0][['label']].count(), 1)[0]\n",
    "    \n",
    "    if sfs is True:\n",
    "    # Sequential Forward Floating Selection\n",
    "        sffs = SFS(RandomForestClassifier(n_jobs=-1, random_state=1, n_estimators=40, max_depth=2,\n",
    "                                          class_weight='balanced_subsample'), \n",
    "                   k_features=(10, 30), \n",
    "                   forward=True, \n",
    "                   floating=True, \n",
    "                   scoring=scoring,\n",
    "                   cv=5,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=0)\n",
    "\n",
    "        sffs = sffs.fit(X_train_, y_train)\n",
    "\n",
    "        print('\\nSequential Forward Floating Selection (k=30):')\n",
    "        sffs_score = sffs.k_score_\n",
    "        print('CV Score: %.2f' % sffs_score)\n",
    "\n",
    "        fig1 = plot_sfs(sffs.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "        #plt.ylim([0.8, 1])\n",
    "        plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "        #plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        selected_cols = []\n",
    "        for i, col in enumerate(X_train_.columns):\n",
    "            if i in sffs.k_feature_idx_:\n",
    "                selected_cols.append(col)\n",
    "\n",
    "        X_train_ = X_train_[selected_cols]\n",
    "        X_test_ = X_test_[selected_cols]\n",
    "    else:\n",
    "        selected_cols = X_train_.columns\n",
    "        \n",
    "    # Fitting model:\n",
    "    if model == 'RF':        \n",
    "        parameters = {'n_estimators': [40, 60, 100, 150, 180],\n",
    "                      'max_depth':[2, 3, 4, 5],\n",
    "                      'min_samples_split': [4, 6],\n",
    "                      'min_samples_leaf': [1, 2],\n",
    "                      'ccp_alpha': [0, 0.01, 0.02]}\n",
    "        \n",
    "        clf = RandomForestClassifier(n_jobs=-1, oob_score=True, criterion='gini', \n",
    "                                     random_state=1, class_weight='balanced_subsample')\n",
    "        \n",
    "        gridcv = GridSearchCV(clf, parameters, cv=TimeSeriesSplit(max_train_size=None, n_splits=5), \n",
    "                              scoring=scoring, verbose=1, n_jobs=-1, refit=True)\n",
    "    elif model == 'XGB':\n",
    "        parameters = {'n_estimators': [40, 60, 100, 180, 1000],\n",
    "                      'max_depth':[2, 3, 4, 5],\n",
    "                      'eta': [0.00005, 0.0005],\n",
    "                      'base_score': [0, 0.5, 1],\n",
    "                      'early_stopping_rounds':[5, 10]}#,\n",
    "                      #'scale_pos_weight': [0.5, 0.8, 1]}\n",
    "        \n",
    "        clf = XGBClassifier(objective='binary:logistic', predictor='gpu_predictor',\n",
    "                            random_state=1, min_child_weight=2, scale_pos_weight=pos_ratio)\n",
    "    \n",
    "        # As this is a time-series problem, we do not shuffle samples for Cross-Validation,\n",
    "        # and test always with newer registers:\n",
    "        gridcv = GridSearchCV(clf, parameters, cv=TimeSeriesSplit(max_train_size=None, n_splits=3), \n",
    "                              verbose=1, n_jobs=-1, refit=True, scoring=scoring, return_train_score=True)\n",
    "\n",
    "    gridcv.fit(X_train_, y_train)\n",
    "\n",
    "    # Results:\n",
    "    best_estimator = gridcv.best_estimator_\n",
    "    #cv_results, cv_results_index = gridcv.cv_results_, gridcv.best_index_\n",
    "    \n",
    "    scores = cross_val_score(best_estimator, X_train_, y_train,\n",
    "                             cv=TimeSeriesSplit(max_train_size=None, n_splits=3), scoring='f1')\n",
    "    print(\"Train F1-score: \", scores.mean())\n",
    "    scores = cross_val_score(best_estimator, X_train_, y_train,\n",
    "                             cv=TimeSeriesSplit(max_train_size=None, n_splits=3), scoring='accuracy')\n",
    "    print(\"Train accuracy: \", scores.mean())\n",
    "    scores = cross_val_score(best_estimator, X_train_, y_train,\n",
    "                             cv=TimeSeriesSplit(max_train_size=None, n_splits=3), scoring='roc_auc')\n",
    "    print(\"Train AUC: \", scores.mean())\n",
    "\n",
    "\n",
    "    print(\"Train best score %.2f\" % gridcv.best_score_)\n",
    "    print('Best parameters: {}'.format(gridcv.best_params_))\n",
    "\n",
    "    predictions = best_estimator.predict(X_test_)\n",
    "    accuracy = accuracy_score(y_test, predictions)*100\n",
    "    print(\"Accuracy: %.2f%%\" % accuracy)\n",
    "    y_pred = gridcv.predict_proba(X_test_)[:,1]\n",
    "    print(\"AUC: %.2f\" % roc_auc_score(y_test, y_pred))\n",
    "    if model == 'RF':\n",
    "        print(\"OOB Score: %.2f\" % best_estimator.oob_score_)\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, predictions)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(conf_mat)\n",
    "    #print(gridcv.cv_results_)\n",
    "    \n",
    "    with plt.style.context('seaborn-poster'):\n",
    "        features = X_train_.columns\n",
    "        importances = np.array([importance for importance in best_estimator.feature_importances_ if importance > 0])\n",
    "        indices = np.argsort(importances)\n",
    "\n",
    "        plt.title('Feature Importances')\n",
    "        plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "        plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "        plt.xlabel('Relative Importance')\n",
    "        plt.show()\n",
    "    \n",
    "    return best_estimator, selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing variables\n",
    "def choose_variables(X_train, X_test, var_type='technical'):\n",
    "    \"\"\"\n",
    "    Selects variables distinguishing between technical indicators and sentiment indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pandas dataframe\n",
    "        Train dataframe.\n",
    "    X_test: pandas dataframe\n",
    "        Test dataframe.\n",
    "    var_type: str\n",
    "        One of 'technical', 'sentiment' or 'all'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Train and test dataframes with selected variables.\n",
    "    \"\"\"\n",
    "    sentiment_cols = [col for col in X_test.columns \n",
    "                      if col.endswith(('USA', 'US500', 'USD')) \n",
    "                      or col.startswith(('pc', 'cross'))]\n",
    "    if var_type == 'technical':\n",
    "        # only technical indicators or financial variables\n",
    "        X_train_ = X_train[[col for col in X_train.columns if col not in sentiment_cols]]\n",
    "        X_test_ = X_test[[col for col in X_test.columns if col not in sentiment_cols]]\n",
    "    elif var_type == 'sentiment':\n",
    "        # only sentiment variables\n",
    "        X_train_ = X_train[sentiment_cols]\n",
    "        X_test_ = X_test[sentiment_cols]\n",
    "    elif var_type == 'all':\n",
    "        X_train_ = X_train\n",
    "        X_test_ = X_test\n",
    "\n",
    "    return X_train_, X_test_\n",
    "\n",
    "X_train_, X_test_ = choose_variables(X_train, X_test, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving here the sffs selections for different configurations\n",
    "\n",
    "# without stop loss and profit taking limits\n",
    "features_all = [ 'ROC',\n",
    "                 'ADX',\n",
    "                 'Daily Return',\n",
    "                 'fear_US500',\n",
    "                 'fundamentalStrength_US500',\n",
    "                 'optimism_USD',\n",
    "                 'surprise_USD',\n",
    "                 'timeUrgency_USD',\n",
    "                 'longShort_USD',\n",
    "                 'bondDefault_USA',\n",
    "                 'bondPriceForecast_USA',\n",
    "                 'interestRates_USA',\n",
    "                 'cross_stockIndexSentiment_USA' ] #all selection\n",
    "features_sent = ['fear_US500',\n",
    "                 'fundamentalStrength_US500',\n",
    "                 'optimism_USD',\n",
    "                 'surprise_USD',\n",
    "                 'longShort_USD',\n",
    "                 'priceForecast_USD',\n",
    "                 'stockIndexStress_USA',\n",
    "                 'bondUncertainty_USA',\n",
    "                 'bondPriceForecast_USA',\n",
    "                 'interestRates_USA',\n",
    "                 'cross_stockIndexSentiment_USA'] #sentiment selection\n",
    "\n",
    "# with stop loss and profit taking limits\n",
    "features_ptsl_all = ['Stoch',\n",
    "                     'Williams',\n",
    "                     'MACD',\n",
    "                     'ADX',\n",
    "                     'Close/Open',\n",
    "                     'sp_cross_Adj Close',\n",
    "                     'longShortForecast_US500',\n",
    "                     'analystRating_US500',\n",
    "                     'dividends_US500',\n",
    "                     'stress_USD',\n",
    "                     'stockIndexPriceDirection_USA',\n",
    "                     'stockIndexPriceForecast_USA',\n",
    "                     'cross_stockIndexSentiment_USA'] # pt sl all selection\n",
    "features_ptsl_sent = ['longShortForecast_US500',\n",
    "                     'priceForecast_US500',\n",
    "                     'analystRating_US500',\n",
    "                     'trust_USD',\n",
    "                     'stress_USD',\n",
    "                     'surprise_USD',\n",
    "                     'timeUrgency_USD',\n",
    "                     'longShort_USD',\n",
    "                     'volatility_USD',\n",
    "                     'stockIndexPriceDirection_USA',\n",
    "                     'bondUncertainty_USA',\n",
    "                     'interestRates_USA',\n",
    "                     'cross_stockIndexSentiment_USA'] # pt sl sentiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying first model for setting the side (sign) of the bet (buy or sell)\n",
    "first_model, features = apply_model(train_labels, test_labels, \n",
    "                                    X_train_[features_all], X_test_[features_all], \n",
    "                                    'XGB', 'neg_log_loss', sfs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating meta-labels with previous forecast\n",
    "# these will be the labels for the next model\n",
    "tbm_train = tbm_train.reindex(X_train_.index)\n",
    "tbm_test = tbm_test.reindex(X_test_.index)\n",
    "\n",
    "tbm_train['side'] = first_model.predict(X_train_[features_all]) # outcome of first model\n",
    "tbm_test['side'] = first_model.predict(X_test_[features_all])\n",
    "\n",
    "metalabels_train = get_bins(tbm_train, sp500_train['Adj Close'])\n",
    "metalabels_test = get_bins(tbm_test, sp500_test['Adj Close'])\n",
    "metalabels_train['label'] = metalabels_train.bin\n",
    "metalabels_test['label'] = metalabels_test.bin\n",
    "metalabels_train.dropna(inplace=True)\n",
    "metalabels_test.dropna(inplace=True)\n",
    "\n",
    "X_train_ = X_train_.reindex(metalabels_train.index)\n",
    "X_test_ = X_test_.reindex(metalabels_test.index)\n",
    "X_train_['side'] = metalabels_train.side\n",
    "X_test_['side'] = metalabels_test.side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metalabels_train.groupby('label').count())\n",
    "display(metalabels_test.groupby('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
